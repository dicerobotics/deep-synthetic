Authored by by Arshad MA

## Workflow: Enhancing Synthetic MWIR Image Realism

### 1. Data Preparation
Collected a set of real MWIR images as ground truth.

Used Cycle-Consistent Unsupervised Translation (CUT) to translate:

Real MWIR → Simulator domain, producing simulator-style equivalents of real MWIR images.

This allowed the creation of paired data:
(Simulated MWIR, Real MWIR)
where the simulated image is the CUT-translated version of the real MWIR.

These aligned pairs are used to train the Pix2Pix model.


### 2. Pix2Pix Training (Initial)
Trained a Pix2Pix model to map:
Simulated MWIR → Realistic MWIR

Objective: Minimize a combination of:

- Adversarial loss (from the GAN framework).

- L1 reconstruction loss (between generated and real MWIR).

### 3. Classifier + Grad-CAM Setup
A ResNet-based classifier is trained on real MWIR images to perform a downstream task (e.g., classification).

Grad-CAM is used to extract class-discriminative attention heatmaps from:

Real MWIR images (ground truth attention).

Pix2Pix-generated images (attention on translated fakes).



### 4. Grad-CAM Consistency Feedback
Compute the difference between Grad-CAM heatmaps of real and translated images.

Introduce a heatmap consistency loss (e.g., L1 or SSIM between Grad-CAM maps).

This encourages the generator to preserve semantically important features recognizable by the classifier.


### 5. Feedback Training Loop
Retrain or fine-tune the Pix2Pix Generator using a combined loss:

``` java
Total Loss = GAN Loss + λ1 * L1 Loss + λ2 * GradCAM Consistency Loss
```
`λ1` and `λ2` balance fidelity and semantic alignment.

### 6. Iterate
Optionally fine-tune the classifier with improved/generated data.

Iterate this process to progressively improve realism and semantic utility.


## Final Output
A Pix2Pix Generator that produces MWIR images that:

Are visually realistic and structurally faithful to real MWIR data.

Retain semantically meaningful features, as validated by Grad-CAM similarity.



# Train and test the CUT Model
Objective: Transferring style from OktalSE simulated images onto MWIR real images and vice versa.

``` bash
cd ./scripts/
./run_real2cut.sh
./run_sim2cut.sh
```

**Output**  
`/workspace/deep-synthetic/results/mwir_real2cut/test_latest/images/ `   
`/workspace/deep-synthetic/results/mwir_sim2cut/test_latest/images/ `   

### **Nomenclature**
**Experiments**  
`mwir_real2cut`: Transferring style from OktalSE simulated images onto MWIR real images.  
`mwir_sim2cut`: Transferring style from MWIR real images onto OktalSE simulated images.  

**Images**  
`mwir_real` : Real MWIR Images from the SENSIAC Dataset placed in datasets/real_A for training.  
`mwir_sim`  : Simulated MWIR image generated with OktalSE simulator placed in datasets/real_B for training.  
`mwir_real2cut` : Images produced by CUT by transferring style of mwir_sim onto mwir_real.  
`mwir_sim2cut`  : Images produced by CUT by transferring style of mwir_real onto mwir_sim.  

### **Directory contents**  
**Experiment:** mwir_real2cut   
`datasets/mwir_real2cut/train_A`: mwir_real    
`datasets/mwir_real2cut/train_B`: mwir_sim    
`datasets/mwir_real2cut/test_A`: mwir_real (unscene during training)  
`datasets/mwir_real2cut/test_B`: mwir_sim  (unscene during training)  
`results/mwir_real2cut/test_lastest/images/real_A/`:   
    - Contains the first `n` images from testA/ — these is your actual input to the CUT model (e.g. mwir_real). `n` defaults to 50  
`results/mwir_real2cut/test_lastest/images/fake_B/`:  
    - Contains the generated outputs from those `n` real_A images. `n` defaults to 50  
`results/mwir_real2cut/test_lastest/images/real_B/`:  
    - Contains `n` random images from testB/, not trainB/, and not aligned with real_A. `n` defaults to 50  

| Folder   | Source Directory      | Purpose                                                 |
| -------- | --------------------- | ------------------------------------------------------- |
| `real_A` | `datasets/.../testA/` | Input images to the model (mwir_real)                   |
| `fake_B` | Generated by model    | Style-transferred with CUT                              |
| `real_B` | `datasets/.../testB/` | Random reference images from target domain B (mwir_sim) |


**Experiment:** mwir_sim2cut  
`datasets/mwir_sim2cut/train_A`: mwir_sim  
`datasets/mwir_sim2cut/train_B`: mwir_real  
`datasets/mwir_sim2cut/test_A`: mwir_sim (unscene during training)   
`datasets/mwir_sim2cut/test_B`: mwir_real (unscene during training)  
`results/mwir_sim2cut/test_lastest/images/real_A/`:  
    - Contains the first `n` images from testA/ — this is your actual input to the CUT model (e.g. mwir_sim). `n` defaults to 50  
`results/mwir_sim2cut/test_lastest/images/fake_B/`:  
    - Contains the generated outputs from those `n` real_A images. `n` defaults to 50  
`results/mwir_sim2cut/test_lastest/images/real_B/`:  
    - Contains `n` random images from testB/, not trainB/, and not aligned with real_A. `n` defaults to 50  

| Folder   | Source Directory      | Purpose                                                  |
| -------- | --------------------- | -------------------------------------------------------- |
| `real_A` | `datasets/.../testA/` | Input images to the model (mwir_sim)                     |
| `fake_B` | Generated by model    | Style-transferred with CUT                               |
| `real_B` | `datasets/.../testB/` | Random reference images from target domain B (mwir_real) |


**Important Notes**:   
CUT is designed for unpaired image-to-image translation.  
The real_B/ folder in the results is just for qualitative comparison — it does not correspond to any particular real_A image unless you're using a paired dataset (which CUT doesn't require).  


### **Image similarity metrices with good result ranges**
| Metric                                                | Based On                       | Measures                       | Robust To                              | Best For               | **Typical Range**              | **Good Result**                                                     |
| ----------------------------------------------------- | ------------------------------ | ------------------------------ | -------------------------------------- | ---------------------- | ------------------------------ | ------------------------------------------------------------------- |
| **PSNR** (Peak Signal-to-Noise Ratio)                 | Pixel-wise error (log scale)   | Pixel-level fidelity           | None (sensitive to small shifts/noise) | Compression, denoising | **\[20 – 50] dB**              | **>30 dB** (acceptable), **>40 dB** (high quality)                  |
| **LPIPS** (Learned Perceptual Image Patch Similarity) | Deep network features          | Perceptual/semantic similarity | Misalignment, texture, color shifts    | GANs, synthesis        | **\[0 – 1]** (lower is better) | **<0.3** (good), **<0.2** (very close), **<0.1** (almost identical) |
| **SSIM** (Structural Similarity Index)                | Luminance, contrast, structure | Structural similarity          | Noise, brightness shifts               | General image quality  | **\[0 – 1]**                   | **>0.85** (good), **>0.95** (very good)                             |


**Our Results**  
Experiment: `mwir_real2cut`  
Samples: 50 (Test set)  

| Comparison              | Value     |
| ----------------------- | --------- |
|PSNR (fake_B vs real_B)  | 16.23 dB  |
|LPIPS (fake_B vs real_B) | 0.3723    | 
|SSIM (fake_B vs real_B)  | 0.6455    |  
| SSIM (real_A vs fake_B) | 0.7259    |  

Samples: 9200 (Complete dataset: train set and test set) 

| Comparison              | Value   |
| ----------------------- | -----   |
| PSNR (fake_B vs real_B) | 13.91 dB|  
| LPIPS (fake_B vs real_B)| 0.3865  |
| SSIM (fake_B vs real_B) | 0.5578  |
| SSIM (real_A vs fake_B) | 0.6943  |


# How to integrate Grad-CAM in Pix2Pix training?
### Basic workflow

**1. Pix2Pix generator**:
Input: source image (e.g., segmentation mask)
Output: generated image (fake photo)

**2. Discriminator**:
Tries to distinguish generated vs real images conditioned on the input

**3. Classifier + Grad-CAM**:
Pretrained classifier frozen (e.g., ResNet trained on real photos)
For each generated image, compute Grad-CAM map for the predicted or target class.

**4. Grad-CAM loss**:
Compare Grad-CAM maps of generated images to real target images’ Grad-CAM maps (or use a target pattern)
This ensures generated images activate the classifier similarly to real images.

**5. Loss to minimize for generator**:

- GAN adversarial loss (Pix2Pix original)

- Reconstruction loss (e.g., L1 between generated & target image)

- Grad-CAM loss (new) to align important regions
